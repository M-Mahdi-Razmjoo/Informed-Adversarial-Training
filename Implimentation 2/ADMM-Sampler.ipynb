{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4481d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.1\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 5e-4\n",
    "EPOCHS = 100\n",
    "K_PERCENTAGE = 1\n",
    "K_SAMPLES = int(BATCH_SIZE * K_PERCENTAGE)\n",
    "\n",
    "EPSILON, ALPHA = 8/255, 2/255\n",
    "PGD_STEPS_TRAIN, PGD_STEPS_EVAL = 10, 20\n",
    "\n",
    "print(f\"K = {K_SAMPLES}, PGD steps (train/eval) = {PGD_STEPS_TRAIN}/{PGD_STEPS_EVAL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35647a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data loading and transformations for CIFAR-10\n",
    "'''\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "transform_test = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(\"Loaded CIFAR-10:\", len(train_dataset), \"train,\", len(test_dataset), \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35179f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Utility functions for ADMM projection operations\n",
    "'''\n",
    "def project_shifted_lp_ball_torch(x, p=2, eps=1e-9):\n",
    "    '''\n",
    "    Projects x onto the shifted Lp ball of radius (n^(1/p))/2, where n is the number of elements in x.\n",
    "    '''\n",
    "    orig_shape = x.shape\n",
    "    x = x.reshape(-1)\n",
    "    shift_vec = 0.5 * torch.ones_like(x, device=x.device)\n",
    "    shift_x = x - shift_vec\n",
    "    normp_shift = torch.norm(shift_x, p=p)\n",
    "    target = (x.numel() ** (1.0 / p)) / 2.0\n",
    "    if normp_shift.item() < eps:\n",
    "        return shift_vec.reshape(orig_shape)\n",
    "    xp = (target / (normp_shift + 1e-20)) * shift_x + shift_vec\n",
    "    return xp.reshape(orig_shape)\n",
    "\n",
    "def project_cardinality_topk_torch(x, k):\n",
    "    '''\n",
    "    Projects x onto the top-k cardinality set, returning a binary tensor of the same shape as x.\n",
    "    '''\n",
    "    x_flat = x.reshape(-1)\n",
    "    k = int(k)\n",
    "    if k <= 0:\n",
    "        return torch.zeros_like(x_flat).reshape(x.shape)\n",
    "    _, idx = torch.topk(x_flat, k)\n",
    "    y = torch.zeros_like(x_flat)\n",
    "    y[idx] = 1.0\n",
    "    return y.reshape(x.shape)\n",
    "\n",
    "def admm_selection_discrete_torch(V, d, all_params=None, warm_x0=None, device=device):\n",
    "    \"\"\"\n",
    "    ADMM solver that uses discrete projection for y1 (top-k) at each iteration.\n",
    "    V: 1D tensor (losses) shape (n,)\n",
    "    d: k (cardinality)\n",
    "    all_params: dict of ADMM params\n",
    "    Returns binary-like selection vector y1 (float tensor with 0/1s) and x_sol continuous\n",
    "    \"\"\"\n",
    "    initial_params = {\n",
    "        'stop_threshold':1e-4, 'gamma_val':1.0, 'rho_change_step':5,\n",
    "        'max_iters':200, 'initial_rho':50.0, 'learning_fact':1.005,\n",
    "        'projection_lp':2, 'eps':1e-9\n",
    "    }\n",
    "    if all_params is None:\n",
    "        all_params = initial_params\n",
    "    else:\n",
    "        for k in initial_params:\n",
    "            if k not in all_params:\n",
    "                all_params[k] = initial_params[k]\n",
    "\n",
    "    V = V.reshape(-1).to(device).float()\n",
    "    n = V.numel()\n",
    "    k_val = int(d)\n",
    "\n",
    "    # warm start\n",
    "    if warm_x0 is not None and warm_x0.shape[0] == n:\n",
    "        x_sol = warm_x0.reshape(-1).to(device).float().clone()\n",
    "    else:\n",
    "        x_sol = torch.rand(n, device=device, dtype=torch.float32)\n",
    "\n",
    "    y1 = x_sol.clone()\n",
    "    y2 = x_sol.clone()\n",
    "    z1 = torch.zeros_like(y1, device=device)\n",
    "    z2 = torch.zeros_like(y2, device=device)\n",
    "    z3 = torch.zeros(1, device=device)\n",
    "\n",
    "    rho1 = rho2 = rho3 = float(all_params['initial_rho'])\n",
    "    gamma_val = float(all_params['gamma_val'])\n",
    "    max_iters = int(all_params['max_iters'])\n",
    "    stop_threshold = float(all_params['stop_threshold'])\n",
    "    p = float(all_params['projection_lp'])\n",
    "    learning_fact = float(all_params['learning_fact'])\n",
    "    eps = float(all_params['eps'])\n",
    "    u = torch.ones(n, device=device, dtype=torch.float32)\n",
    "\n",
    "    for it in range(max_iters):\n",
    "        y1 = project_cardinality_topk_torch(x_sol + z1 / rho1, k_val)\n",
    "        y2 = project_shifted_lp_ball_torch(x_sol + z2 / rho2, p=p)\n",
    "        q = (V - z1 - z2 - z3 * u) + rho1 * y1 + rho2 * y2 + rho3 * (d * u)\n",
    "\n",
    "        alpha = float(rho1 + rho2)\n",
    "        beta = float(rho3)\n",
    "        denom = alpha * (alpha + beta * n) + 1e-30\n",
    "        factor = beta / denom\n",
    "        sum_q = torch.sum(q)\n",
    "        x_new = q / alpha - factor * sum_q * u\n",
    "        x_sol = x_new\n",
    "\n",
    "        z1 = z1 + gamma_val * rho1 * (x_sol - y1)\n",
    "        z2 = z2 + gamma_val * rho2 * (x_sol - y2)\n",
    "        z3 = z3 + gamma_val * rho3 * (torch.sum(x_sol) - float(d))\n",
    "\n",
    "        if (it + 1) % int(all_params['rho_change_step']) == 0:\n",
    "            rho1 *= learning_fact\n",
    "            rho2 *= learning_fact\n",
    "            rho3 *= learning_fact\n",
    "            gamma_val = max(gamma_val * 0.95, 1.0)\n",
    "\n",
    "        norm_x = torch.norm(x_sol) if torch.norm(x_sol) > 0 else torch.tensor(1.0, device=device)\n",
    "        res1 = torch.norm(x_sol - y1) / (norm_x + eps)\n",
    "        res2 = torch.norm(x_sol - y2) / (norm_x + eps)\n",
    "        if max(res1.item(), res2.item()) <= stop_threshold:\n",
    "            break\n",
    "\n",
    "    sel = torch.nonzero(y1.reshape(-1) >= 0.5).reshape(-1)\n",
    "    if sel.numel() != k_val:\n",
    "        _, sel = torch.topk(x_sol, k_val)\n",
    "    return sel, x_sol.detach()\n",
    "\n",
    "class ADMM_Discrete_Solver_Torch:\n",
    "    '''\n",
    "    ADMM solver for discrete selection problems using PyTorch.\n",
    "    '''\n",
    "    def __init__(self, n, k, admm_params=None, device=device, use_warmstart=True):\n",
    "        self.n = n\n",
    "        self.k = int(k)\n",
    "        self.device = device\n",
    "        self.admm_params = admm_params if admm_params is not None else {\n",
    "            'max_iters':200, 'stop_threshold':1e-4, 'initial_rho':50.0, 'projection_lp':2,\n",
    "            'rho_change_step':5, 'learning_fact':1.005, 'gamma_val':1.0, 'eps':1e-9\n",
    "        }\n",
    "        self.last_x = None\n",
    "        self.use_warmstart = use_warmstart\n",
    "\n",
    "    def solve(self, V):\n",
    "        '''\n",
    "        Solve the discrete selection problem using ADMM.\n",
    "        '''\n",
    "        warm = self.last_x if (self.use_warmstart and self.last_x is not None and self.last_x.shape[0] == self.n) else None\n",
    "        sel, x_sol = admm_selection_discrete_torch(V, d=self.k, all_params=self.admm_params, warm_x0=warm, device=self.device)\n",
    "        self.last_x = x_sol.clone()\n",
    "        return sel, x_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7db541",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PGD attack function for adversarial training and evaluation\n",
    "'''\n",
    "def pgd_attack(model, images, labels, epsilon, alpha, iters):\n",
    "    '''\n",
    "    PGD attack implementation for adversarial training and evaluation.\n",
    "    '''\n",
    "    images = images.clone().detach().to(device)\n",
    "    labels = labels.to(device)\n",
    "    orig = images.clone().detach()\n",
    "    for _ in range(iters):\n",
    "        images.requires_grad = True\n",
    "        outs = model(images)\n",
    "        loss = F.cross_entropy(outs, labels)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        images = images + alpha * images.grad.sign()\n",
    "        eta = torch.clamp(images - orig, -epsilon, epsilon)\n",
    "        images = torch.clamp(orig + eta, 0.0, 1.0).detach()\n",
    "    return images\n",
    "\n",
    "def compute_overlap_indices(losses, indices_a, indices_b):\n",
    "    '''\n",
    "    Utility to compute overlap between two sets of indices.\n",
    "    '''\n",
    "    sa = set(indices_a.cpu().numpy().tolist())\n",
    "    sb = set(indices_b.cpu().numpy().tolist())\n",
    "    return len(sa & sb) / float(len(sa))\n",
    "\n",
    "admm_discrete_solver = ADMM_Discrete_Solver_Torch(n=2*BATCH_SIZE, k=K_SAMPLES, admm_params=None, device=device, use_warmstart=True)\n",
    "\n",
    "def select_admm_discrete_wrapper(losses, k):\n",
    "    '''\n",
    "    losses: tensor on device, shape (2*BATCH_SIZE,)\n",
    "    '''\n",
    "    sel, scores = admm_discrete_solver.solve(losses)\n",
    "    return sel, scores\n",
    "\n",
    "def train_epoch_admm_discrete(model, optimizer, data_loader, admm_solver, k):\n",
    "    '''\n",
    "    Train epoch that returns mean overlap vs TopK for the epoch.\n",
    "    '''\n",
    "    model.train()\n",
    "    overlaps = []\n",
    "    for clean_images, labels in data_loader:\n",
    "        clean_images, labels = clean_images.to(device), labels.to(device)\n",
    "        adv_images = pgd_attack(model, clean_images, labels, EPSILON, ALPHA, PGD_STEPS_TRAIN)\n",
    "        combined_images = torch.cat([clean_images, adv_images], dim=0)\n",
    "        combined_labels = torch.cat([labels, labels], dim=0)\n",
    "        with torch.no_grad():\n",
    "            outs = model(combined_images)\n",
    "            losses = F.cross_entropy(outs, combined_labels, reduction='none')\n",
    "\n",
    "        topk_idx = torch.topk(losses, k).indices\n",
    "\n",
    "        sel_idx, _ = admm_solver.solve(losses) \n",
    "        sel_idx = sel_idx.to(device)\n",
    "\n",
    "        overlap = compute_overlap_indices(losses, topk_idx, sel_idx)\n",
    "        overlaps.append(overlap)\n",
    "\n",
    "        final_images = combined_images[sel_idx]\n",
    "        final_labels = combined_labels[sel_idx]\n",
    "        if final_images.size(0) > 0:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(final_images)\n",
    "            loss = F.cross_entropy(predictions, final_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    mean_overlap = float(np.mean(overlaps)) if overlaps else 0.0\n",
    "    return mean_overlap\n",
    "\n",
    "def evaluate(model, data_loader, attack_fn=None):\n",
    "    '''\n",
    "    Evaluate the model on the test set.\n",
    "    '''\n",
    "    model.eval()\n",
    "    total_correct, total = 0, 0\n",
    "    for images, labels in data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        if attack_fn is not None:\n",
    "            images = pgd_attack(model, images, labels, EPSILON, ALPHA, PGD_STEPS_EVAL)\n",
    "        with torch.no_grad():\n",
    "            outs = model(images)\n",
    "            _, preds = torch.max(outs, 1)\n",
    "            total += labels.size(0)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "    return 100.0 * total_correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705d7a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Checkpoint utilities for saving and loading model state\n",
    "'''\n",
    "def save_checkpoint(epoch, model, optimizer, scheduler, history, path):\n",
    "    '''Saves checkpoint to a file.'''\n",
    "    dir_name = os.path.dirname(path)\n",
    "    \n",
    "    if dir_name and not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    \n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'history': history\n",
    "    }, path)\n",
    "    print(f\"Checkpoint saved to {path}\")\n",
    "\n",
    "def load_checkpoint(path, model, optimizer, scheduler):\n",
    "    '''Loads checkpoint from a file.'''\n",
    "    start_epoch = 0\n",
    "    history = {'epoch':[], 'std_acc':[], 'robust_acc':[], 'cumulative_time':[], 'epoch_time':[], 'overlap':[]}\n",
    "    \n",
    "    if path and os.path.exists(path):\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        for key in history.keys():\n",
    "            if key not in checkpoint['history']:\n",
    "                checkpoint['history'][key] = []\n",
    "        history = checkpoint['history']\n",
    "        print(f\"Checkpoint loaded from {path}. Resuming from epoch {start_epoch + 1}\")\n",
    "    else:\n",
    "        print(\"No checkpoint found. Starting from scratch.\")\n",
    "        \n",
    "    return start_epoch, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d4fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Experiment runner for ADMM-discrete training\n",
    "'''\n",
    "def run_experiment_admm_discrete(method_name=\"ADMM_discrete_test\", checkpoint_to_load=None, new_checkpoint_base='admm_discrete_ckpt'):\n",
    "    print(\"Running:\", method_name)\n",
    "    model = models.resnet18(weights=None, num_classes=10).to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[75,90], gamma=0.1)\n",
    "\n",
    "    start_epoch = 0\n",
    "    history = {}\n",
    "    if checkpoint_to_load:\n",
    "        start_epoch, history = load_checkpoint(checkpoint_to_load, model, optimizer, scheduler)\n",
    "    else:\n",
    "        start_epoch, history = load_checkpoint(None, model, optimizer, scheduler)\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(start_epoch, EPOCHS):\n",
    "        t0 = time.time()\n",
    "        \n",
    "        mean_overlap = train_epoch_admm_discrete(model, optimizer, train_loader, admm_discrete_solver, K_SAMPLES)\n",
    "        \n",
    "        std_acc = evaluate(model, test_loader, attack_fn=None)\n",
    "        robust_acc = evaluate(model, test_loader, attack_fn=pgd_attack)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_time = time.time() - t0\n",
    "        cumulative_time = time.time() - start_time\n",
    "        \n",
    "        history['epoch'].append(epoch + 1)\n",
    "        history['std_acc'].append(std_acc)\n",
    "        history['robust_acc'].append(robust_acc)\n",
    "        history['cumulative_time'].append(cumulative_time)\n",
    "        history['epoch_time'].append(epoch_time)\n",
    "        history['overlap'].append(mean_overlap)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Std Acc: {std_acc:.2f}% | Robust Acc: {robust_acc:.2f}% | Overlap(topk): {mean_overlap:.3f} | Epoch Time: {epoch_time:.1f}s\")\n",
    "\n",
    "        if (epoch + 1) % 10 == 0 or (epoch + 1) == EPOCHS:\n",
    "            save_checkpoint(epoch + 1, model, optimizer, scheduler, history, f\"{new_checkpoint_base}_epoch_{epoch+1}.pth\")\n",
    "\n",
    "    fname = f\"results_{method_name}.json\"\n",
    "    final_results = {\n",
    "        'final_std_acc': history['std_acc'][-1],\n",
    "        'final_robust_acc': history['robust_acc'][-1],\n",
    "        'total_training_time': history['cumulative_time'][-1]\n",
    "    }\n",
    "    \n",
    "    output_data = {\n",
    "        'experiment_name': method_name,\n",
    "        'hyperparameters': {\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'epochs': EPOCHS,\n",
    "            'k_percentage': K_PERCENTAGE,\n",
    "            'epsilon': EPSILON\n",
    "        },\n",
    "        'training_history': history, \n",
    "        'final_summary': final_results\n",
    "    }\n",
    "    \n",
    "    with open(fname, 'w') as f:\n",
    "        json.dump(output_data, f, indent=4)\n",
    "    print(f\"Full experiment results saved to {fname}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "history_admm_discrete = run_experiment_admm_discrete(method_name=f\"ADMM_discrete_k{K_SAMPLES}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
