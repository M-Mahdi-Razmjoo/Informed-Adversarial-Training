{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b7660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This script sets up the training environment for the ADMM discrete method on CIFAR-100.\n",
    "'''\n",
    "\n",
    "import os, time, json\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.1\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 5e-4\n",
    "EPOCHS = 100\n",
    "K_PERCENTAGE = 0.5\n",
    "K_SAMPLES = int(BATCH_SIZE * K_PERCENTAGE)\n",
    "\n",
    "EPSILON, ALPHA = 8/255, 2/255\n",
    "PGD_STEPS_TRAIN, PGD_STEPS_EVAL = 10, 20\n",
    "\n",
    "SEEDS = [42, 43]\n",
    "\n",
    "NUM_CLASSES = 100\n",
    "METHOD_NAME = f\"ADMM_discrete_CIFAR100_k{K_SAMPLES}_bs{BATCH_SIZE}\"\n",
    "OUT_DIR = \"results_runs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Method: {METHOD_NAME}, K={K_SAMPLES}, BATCH_SIZE={BATCH_SIZE}, seeds={SEEDS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa28f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data loading and transformations\n",
    "'''\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "transform_test = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset  = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(\"Loaded CIFAR-100:\", len(train_dataset), \"train,\", len(test_dataset), \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2530c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to project onto the shifted Lp ball and cardinality constraints\n",
    "'''\n",
    "\n",
    "def project_shifted_lp_ball_torch(x, p=2, eps=1e-9):\n",
    "    '''\n",
    "    Projects a tensor onto the shifted Lp ball of radius (x.numel() ** (1.0 / p)) / 2.0.\n",
    "    '''\n",
    "    orig_shape = x.shape\n",
    "    x = x.reshape(-1)\n",
    "    shift_vec = 0.5 * torch.ones_like(x, device=x.device)\n",
    "    shift_x = x - shift_vec\n",
    "    normp_shift = torch.norm(shift_x, p=p)\n",
    "    target = (x.numel() ** (1.0 / p)) / 2.0\n",
    "    if normp_shift.item() < eps:\n",
    "        return shift_vec.reshape(orig_shape)\n",
    "    xp = (target / (normp_shift + 1e-20)) * shift_x + shift_vec\n",
    "    return xp.reshape(orig_shape)\n",
    "\n",
    "def project_cardinality_topk_torch(x, k):\n",
    "    '''\n",
    "    Projects a tensor onto the top-k cardinality constraint.\n",
    "    '''\n",
    "    x_flat = x.reshape(-1)\n",
    "    k = int(k)\n",
    "    if k <= 0:\n",
    "        return torch.zeros_like(x_flat).reshape(x.shape)\n",
    "    _, idx = torch.topk(x_flat, k)\n",
    "    y = torch.zeros_like(x_flat)\n",
    "    y[idx] = 1.0\n",
    "    return y.reshape(x.shape)\n",
    "\n",
    "def admm_selection_discrete_torch(V, d, all_params=None, warm_x0=None, device=device):\n",
    "    '''\n",
    "    ADMM solver for discrete selection problem with cardinality and shifted Lp ball constraints.\n",
    "    '''\n",
    "    initial_params = {\n",
    "        'stop_threshold':1e-4, 'gamma_val':1.0, 'rho_change_step':5,\n",
    "        'max_iters':200, 'initial_rho':50.0, 'learning_fact':1.005,\n",
    "        'projection_lp':2, 'eps':1e-9\n",
    "    }\n",
    "    if all_params is None:\n",
    "        all_params = initial_params\n",
    "    else:\n",
    "        for k in initial_params:\n",
    "            if k not in all_params:\n",
    "                all_params[k] = initial_params[k]\n",
    "\n",
    "    V = V.reshape(-1).to(device).float()\n",
    "    n = V.numel()\n",
    "    k_val = int(d)\n",
    "\n",
    "    if warm_x0 is not None and warm_x0.shape[0] == n:\n",
    "        x_sol = warm_x0.reshape(-1).to(device).float().clone()\n",
    "    else:\n",
    "        x_sol = torch.rand(n, device=device, dtype=torch.float32)\n",
    "\n",
    "    y1 = x_sol.clone()\n",
    "    y2 = x_sol.clone()\n",
    "    z1 = torch.zeros_like(y1, device=device)\n",
    "    z2 = torch.zeros_like(y2, device=device)\n",
    "    z3 = torch.zeros(1, device=device)\n",
    "\n",
    "    rho1 = rho2 = rho3 = float(all_params['initial_rho'])\n",
    "    gamma_val = float(all_params['gamma_val'])\n",
    "    max_iters = int(all_params['max_iters'])\n",
    "    stop_threshold = float(all_params['stop_threshold'])\n",
    "    p = float(all_params['projection_lp'])\n",
    "    learning_fact = float(all_params['learning_fact'])\n",
    "    eps = float(all_params['eps'])\n",
    "    u = torch.ones(n, device=device, dtype=torch.float32)\n",
    "\n",
    "    for it in range(max_iters):\n",
    "        y1 = project_cardinality_topk_torch(x_sol + z1 / rho1, k_val)\n",
    "        y2 = project_shifted_lp_ball_torch(x_sol + z2 / rho2, p=p)\n",
    "\n",
    "        q = (V - z1 - z2 - z3 * u) + rho1 * y1 + rho2 * y2 + rho3 * (d * u)\n",
    "        alpha = float(rho1 + rho2)\n",
    "        beta = float(rho3)\n",
    "        denom = alpha * (alpha + beta * n) + 1e-30\n",
    "        factor = beta / denom\n",
    "        sum_q = torch.sum(q)\n",
    "        x_new = q / alpha - factor * sum_q * u\n",
    "        x_sol = x_new\n",
    "\n",
    "        z1 = z1 + gamma_val * rho1 * (x_sol - y1)\n",
    "        z2 = z2 + gamma_val * rho2 * (x_sol - y2)\n",
    "        z3 = z3 + gamma_val * rho3 * (torch.sum(x_sol) - float(d))\n",
    "\n",
    "        if (it + 1) % int(all_params['rho_change_step']) == 0:\n",
    "            rho1 *= learning_fact\n",
    "            rho2 *= learning_fact\n",
    "            rho3 *= learning_fact\n",
    "            gamma_val = max(gamma_val * 0.95, 1.0)\n",
    "\n",
    "        norm_x = torch.norm(x_sol) if torch.norm(x_sol) > 0 else torch.tensor(1.0, device=device)\n",
    "        res1 = torch.norm(x_sol - y1) / (norm_x + eps)\n",
    "        res2 = torch.norm(x_sol - y2) / (norm_x + eps)\n",
    "        if max(res1.item(), res2.item()) <= stop_threshold:\n",
    "            break\n",
    "\n",
    "    sel = torch.nonzero(y1.reshape(-1) >= 0.5).reshape(-1)\n",
    "    if sel.numel() != k_val:\n",
    "        _, sel = torch.topk(x_sol, k_val)\n",
    "    return sel, x_sol.detach()\n",
    "\n",
    "class ADMM_Discrete_Solver_Torch:\n",
    "    '''\n",
    "    ADMM solver for discrete selection with cardinality and shifted Lp ball constraints using PyTorch.\n",
    "    '''\n",
    "    def __init__(self, n, k, admm_params=None, device=device, use_warmstart=True):\n",
    "        self.n = n\n",
    "        self.k = int(k)\n",
    "        self.device = device\n",
    "        self.admm_params = admm_params if admm_params is not None else {\n",
    "            'max_iters':200, 'stop_threshold':1e-4, 'initial_rho':50.0, 'projection_lp':2,\n",
    "            'rho_change_step':5, 'learning_fact':1.005, 'gamma_val':1.0, 'eps':1e-9\n",
    "        }\n",
    "        self.last_x = None\n",
    "        self.use_warmstart = use_warmstart\n",
    "\n",
    "    def solve(self, V):\n",
    "        '''\n",
    "        Solves the discrete selection problem using ADMM with given parameters.\n",
    "        '''\n",
    "        warm = self.last_x if (self.use_warmstart and self.last_x is not None and self.last_x.shape[0] == self.n) else None\n",
    "        sel, x_sol = admm_selection_discrete_torch(V, d=self.k, all_params=self.admm_params, warm_x0=warm, device=self.device)\n",
    "        self.last_x = x_sol.clone()\n",
    "        return sel, x_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b70334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PGD, selection wrapper, per-class evaluation utilities\n",
    "'''\n",
    "def pgd_attack(model, images, labels, epsilon, alpha, iters):\n",
    "    '''\n",
    "    Performs the PGD attack on the input images.\n",
    "    '''\n",
    "    images = images.clone().detach().to(device)\n",
    "    labels = labels.to(device)\n",
    "    orig = images.clone().detach()\n",
    "    for _ in range(iters):\n",
    "        images.requires_grad = True\n",
    "        outputs = model(images)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        images = images + alpha * images.grad.sign()\n",
    "        eta = torch.clamp(images - orig, -epsilon, epsilon)\n",
    "        images = torch.clamp(orig + eta, 0.0, 1.0).detach()\n",
    "    return images\n",
    "\n",
    "def compute_overlap_indices(losses, indices_a, indices_b):\n",
    "    '''\n",
    "    Computes the overlap between two sets of indices based on their losses.\n",
    "    '''\n",
    "    sa = set(indices_a.cpu().numpy().tolist())\n",
    "    sb = set(indices_b.cpu().numpy().tolist())\n",
    "    if len(sa) == 0: return 0.0\n",
    "    return len(sa & sb) / float(len(sa))\n",
    "\n",
    "def compute_selection_stability(list_of_selected_indices, k):\n",
    "    '''\n",
    "    Computes the average Jaccard index between consecutive selections.\n",
    "    '''\n",
    "    if len(list_of_selected_indices) < 2:\n",
    "        return 1.0\n",
    "    scores = []\n",
    "    prev = None\n",
    "    for sel in list_of_selected_indices:\n",
    "        s = set(sel.cpu().numpy().tolist())\n",
    "        if prev is not None:\n",
    "            scores.append(len(prev & s) / float(k))\n",
    "        prev = s\n",
    "    return float(np.mean(scores)) if len(scores)>0 else 0.0\n",
    "\n",
    "def evaluate_per_class(model, data_loader, attack_fn=None, num_classes=NUM_CLASSES):\n",
    "    '''\n",
    "    Returns overall_acc (float) and per_class_acc (list length=num_classes)\n",
    "    '''\n",
    "    model.eval()\n",
    "    correct_per_class = np.zeros(num_classes, dtype=np.int64)\n",
    "    total_per_class = np.zeros(num_classes, dtype=np.int64)\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for images, labels in data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        if attack_fn is not None:\n",
    "            images = attack_fn(model, images, labels, EPSILON, ALPHA, PGD_STEPS_EVAL)\n",
    "        with torch.no_grad():\n",
    "            outs = model(images)\n",
    "            _, preds = torch.max(outs, 1)\n",
    "            preds_cpu = preds.cpu().numpy()\n",
    "            labels_cpu = labels.cpu().numpy()\n",
    "            total_correct += int((preds_cpu == labels_cpu).sum())\n",
    "            total_samples += labels_cpu.shape[0]\n",
    "            for c in range(num_classes):\n",
    "                mask = (labels_cpu == c)\n",
    "                if mask.sum() > 0:\n",
    "                    correct_per_class[c] += int((preds_cpu[mask] == c).sum())\n",
    "                    total_per_class[c] += int(mask.sum())\n",
    "    overall_acc = 100.0 * total_correct / total_samples if total_samples>0 else 0.0\n",
    "    per_class_acc = []\n",
    "    for c in range(num_classes):\n",
    "        if total_per_class[c] > 0:\n",
    "            per_class_acc.append(100.0 * correct_per_class[c] / total_per_class[c])\n",
    "        else:\n",
    "            per_class_acc.append(None)\n",
    "    return overall_acc, per_class_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb12b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train_epoch + run_experiment_seed (single seed) -> produces and saves a seed JSON\n",
    "'''\n",
    "def train_epoch_admm_discrete_collect(model, optimizer, data_loader, admm_solver, k):\n",
    "    '''\n",
    "    Trains the model for one epoch using ADMM discrete selection and collects overlap and stability metrics.\n",
    "    '''\n",
    "    model.train()\n",
    "    overlaps = []\n",
    "    selected_history = []\n",
    "    for clean_images, labels in data_loader:\n",
    "        clean_images, labels = clean_images.to(device), labels.to(device)\n",
    "        adv_images = pgd_attack(model, clean_images, labels, EPSILON, ALPHA, PGD_STEPS_TRAIN)\n",
    "        combined_images = torch.cat([clean_images, adv_images], dim=0)\n",
    "        combined_labels = torch.cat([labels, labels], dim=0)\n",
    "        with torch.no_grad():\n",
    "            outs = model(combined_images)\n",
    "            losses = F.cross_entropy(outs, combined_labels, reduction='none')\n",
    "\n",
    "        topk_idx = torch.topk(losses, k).indices\n",
    "\n",
    "        sel_idx, _ = admm_solver.solve(losses)\n",
    "        sel_idx = sel_idx.to(device)\n",
    "\n",
    "        overlap = compute_overlap_indices(losses, topk_idx, sel_idx)\n",
    "        overlaps.append(overlap)\n",
    "        selected_history.append(sel_idx.clone())\n",
    "\n",
    "        final_images = combined_images[sel_idx]\n",
    "        final_labels = combined_labels[sel_idx]\n",
    "        if final_images.size(0) > 0:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(final_images)\n",
    "            loss = F.cross_entropy(predictions, final_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    mean_overlap = float(np.mean(overlaps)) if len(overlaps)>0 else 0.0\n",
    "    stability = compute_selection_stability(selected_history, k)\n",
    "    return mean_overlap, stability\n",
    "\n",
    "def run_experiment_seed(seed, method_name=METHOD_NAME, admm_params=None):\n",
    "    '''\n",
    "    Runs a single seed experiment with the given parameters and saves the results to a JSON file.\n",
    "    '''\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    print(f\"\\n=== RUN seed {seed} ===\")\n",
    "    model = models.resnet18(weights=None, num_classes=NUM_CLASSES).to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(0.75*EPOCHS), int(0.9*EPOCHS)], gamma=0.1)\n",
    "\n",
    "    admm_solver_local = ADMM_Discrete_Solver_Torch(n=2*BATCH_SIZE, k=K_SAMPLES, admm_params=admm_params, device=device, use_warmstart=True)\n",
    "\n",
    "    history = {\n",
    "        'epoch': [],\n",
    "        'std_acc': [],\n",
    "        'robust_acc': [],\n",
    "        'epoch_time': [],\n",
    "        'cumulative_time': [],\n",
    "        'overlap': [],\n",
    "        'selection_stability': [],\n",
    "        'per_class_std_acc': [],\n",
    "        'per_class_robust_acc': []\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        t0 = time.time()\n",
    "        mean_overlap, stability = train_epoch_admm_discrete_collect(model, optimizer, train_loader, admm_solver_local, K_SAMPLES)\n",
    "\n",
    "        std_acc, per_class_std = evaluate_per_class(model, test_loader, attack_fn=None, num_classes=NUM_CLASSES)\n",
    "        robust_acc, per_class_robust = evaluate_per_class(model, test_loader, attack_fn=pgd_attack, num_classes=NUM_CLASSES)\n",
    "\n",
    "        scheduler.step()\n",
    "        epoch_time = time.time() - t0\n",
    "        cumulative_time = time.time() - start_time\n",
    "\n",
    "        history['epoch'].append(epoch)\n",
    "        history['std_acc'].append(std_acc)\n",
    "        history['robust_acc'].append(robust_acc)\n",
    "        history['epoch_time'].append(epoch_time)\n",
    "        history['cumulative_time'].append(cumulative_time)\n",
    "        history['overlap'].append(mean_overlap)\n",
    "        history['selection_stability'].append(stability)\n",
    "        history['per_class_std_acc'].append(per_class_std)\n",
    "        history['per_class_robust_acc'].append(per_class_robust)\n",
    "\n",
    "        print(f\"Seed {seed} Epoch {epoch}/{EPOCHS} | Std: {std_acc:.2f}% | Robust: {robust_acc:.2f}% | Overlap: {mean_overlap:.3f} | Stability: {stability:.3f} | EpochTime: {epoch_time:.1f}s\")\n",
    "\n",
    "    out = {\n",
    "        'experiment_name': f\"{method_name}_seed{seed}\",\n",
    "        'hyperparameters': {\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'epochs': EPOCHS,\n",
    "            'k_percentage': K_PERCENTAGE,\n",
    "            'k_samples': K_SAMPLES,\n",
    "            'epsilon': EPSILON\n",
    "        },\n",
    "        'training_history': history,\n",
    "        'final_summary': {\n",
    "            'final_std_acc': history['std_acc'][-1],\n",
    "            'final_robust_acc': history['robust_acc'][-1],\n",
    "            'total_training_time': history['cumulative_time'][-1]\n",
    "        }\n",
    "    }\n",
    "    seed_fname = os.path.join(OUT_DIR, f\"{method_name}_seed{seed}.json\")\n",
    "    with open(seed_fname, 'w') as f:\n",
    "        json.dump(out, f, indent=4)\n",
    "    print(\"Saved seed results to\", seed_fname)\n",
    "    return seed_fname, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075e1c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run all seeds, aggregate results, and save final JSON\n",
    "'''\n",
    "seed_files = []\n",
    "seed_outputs = []\n",
    "admm_params = None\n",
    "\n",
    "for s in SEEDS:\n",
    "    fname, out = run_experiment_seed(s, method_name=METHOD_NAME, admm_params=admm_params)\n",
    "    seed_files.append(fname)\n",
    "    seed_outputs.append(out)\n",
    "\n",
    "min_epochs = min(len(o['training_history']['epoch']) for o in seed_outputs)\n",
    "metrics = ['std_acc','robust_acc','epoch_time','cumulative_time','overlap','selection_stability']\n",
    "\n",
    "agg_history = {'epoch': list(range(1, min_epochs+1))}\n",
    "for m in metrics:\n",
    "    arr = np.array([o['training_history'][m][:min_epochs] for o in seed_outputs], dtype=float)\n",
    "    agg_history[m + '_mean'] = list(np.nanmean(arr, axis=0))\n",
    "    agg_history[m + '_std']  = list(np.nanstd(arr, axis=0, ddof=1))\n",
    "\n",
    "per_class_std = np.array([o['training_history']['per_class_std_acc'][:min_epochs] for o in seed_outputs], dtype=float)\n",
    "per_class_rob = np.array([o['training_history']['per_class_robust_acc'][:min_epochs] for o in seed_outputs], dtype=float)\n",
    "per_class_std_mean = np.nanmean(per_class_std, axis=0).tolist()\n",
    "per_class_std_std  = np.nanstd(per_class_std, axis=0, ddof=1).tolist()\n",
    "per_class_rob_mean = np.nanmean(per_class_rob, axis=0).tolist()\n",
    "per_class_rob_std  = np.nanstd(per_class_rob, axis=0, ddof=1).tolist()\n",
    "\n",
    "aggregate_output = {\n",
    "    'experiment_name': METHOD_NAME + \"_aggregate\",\n",
    "    'seed_files': seed_files,\n",
    "    'hyperparameters': {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'epochs': EPOCHS,\n",
    "        'k_percentage': K_PERCENTAGE,\n",
    "        'k_samples': K_SAMPLES,\n",
    "        'epsilon': EPSILON\n",
    "    },\n",
    "    'training_history_aggregate': agg_history,\n",
    "    'per_class_std_mean_per_epoch': per_class_std_mean,\n",
    "    'per_class_std_std_per_epoch': per_class_std_std,\n",
    "    'per_class_robust_mean_per_epoch': per_class_rob_mean,\n",
    "    'per_class_robust_std_per_epoch': per_class_rob_std,\n",
    "    'final_summary_aggregate': {\n",
    "        'final_std_acc_mean': float(np.nanmean([o['final_summary']['final_std_acc'] for o in seed_outputs])),\n",
    "        'final_std_acc_std' : float(np.nanstd([o['final_summary']['final_std_acc'] for o in seed_outputs], ddof=1)),\n",
    "        'final_robust_acc_mean': float(np.nanmean([o['final_summary']['final_robust_acc'] for o in seed_outputs])),\n",
    "        'final_robust_acc_std' : float(np.nanstd([o['final_summary']['final_robust_acc'] for o in seed_outputs], ddof=1)),\n",
    "        'total_training_time_mean': float(np.nanmean([o['final_summary']['total_training_time'] for o in seed_outputs])),\n",
    "        'total_training_time_std' : float(np.nanstd([o['final_summary']['total_training_time'] for o in seed_outputs], ddof=1))\n",
    "    }\n",
    "}\n",
    "\n",
    "agg_fname = os.path.join(OUT_DIR, f\"{METHOD_NAME}_aggregate.json\")\n",
    "with open(agg_fname, 'w') as f:\n",
    "    json.dump(aggregate_output, f, indent=4)\n",
    "\n",
    "print(\"Saved aggregate results to\", agg_fname)\n",
    "print(\"Seed files:\", seed_files)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
