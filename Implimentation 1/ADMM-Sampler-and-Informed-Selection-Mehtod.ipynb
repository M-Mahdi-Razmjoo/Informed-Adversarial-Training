{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c5ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from scipy.sparse import csc_matrix\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import importlib\n",
    "\n",
    "lpbox_admm_code = \"\"\"\n",
    "import numpy as np\n",
    "import scipy.sparse.linalg as linalg\n",
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "def ADMM_bqp_linear_eq(A,b,C,d, all_params=None):\n",
    "    initial_params = {'stop_threshold':1e-4,'gamma_val':1.6,'rho_change_step':5, \\\n",
    "    'max_iters':1e3,'initial_rho':25,'learning_fact':1+1/100,'x0':None,'pcg_tol':1e-4, 'pcg_maxiters':1e3, 'projection_lp':2}\n",
    "    if all_params==None: all_params = initial_params\n",
    "    else:\n",
    "        for k in initial_params.keys():\n",
    "            if k not in all_params.keys(): all_params[k] = initial_params[k]\n",
    "    n = b.size\n",
    "    stop_threshold, max_iters, initial_rho = all_params['stop_threshold'], all_params['max_iters'], all_params['initial_rho']\n",
    "    rho_change_step, gamma_val, learning_fact = all_params['rho_change_step'], all_params['gamma_val'], all_params['learning_fact']\n",
    "    projection_lp, pcg_tol, pcg_maxiters = all_params['projection_lp'], all_params['pcg_tol'], all_params['pcg_maxiters']\n",
    "    x_sol = all_params['x0'] if all_params['x0'] is not None else np.random.rand(n, 1)\n",
    "    y1, y2 = x_sol.copy(), x_sol.copy()\n",
    "    z1, z2, z3 = np.zeros_like(y1), np.zeros_like(y2), np.zeros_like(d)\n",
    "    rho1, rho2, rho3 = initial_rho, initial_rho, initial_rho\n",
    "    Csq = csc_matrix(C.transpose() @ C)\n",
    "    for iter_num in range(int(max_iters)):\n",
    "        y1 = project_box(x_sol + z1 / rho1)\n",
    "        y2 = project_shifted_Lp_ball(x_sol + z2 / rho2, projection_lp)\n",
    "        diag_rho = csc_matrix(( (rho1 + rho2) * np.ones(n), (range(n), range(n)) ), shape=(n, n))\n",
    "        M = 2 * A + rho3 * Csq + diag_rho\n",
    "        q = -(b + z1 + z2 + C.transpose() @ z3) + rho1 * y1 + rho2 * y2 + rho3 * C.transpose() @ d\n",
    "        x_sol, cg_flag = linalg.cg(M, q, x_sol, atol=pcg_tol, maxiter=int(pcg_maxiters))\n",
    "        x_sol = x_sol.reshape(-1, 1)\n",
    "        z1 += gamma_val * rho1 * (x_sol - y1)\n",
    "        z2 += gamma_val * rho2 * (x_sol - y2)\n",
    "        z3 += gamma_val * rho3 * (C @ x_sol - d)\n",
    "        if (iter_num + 1) % rho_change_step == 0:\n",
    "            rho1, rho2, rho3 = rho1 * learning_fact, rho2 * learning_fact, rho3 * learning_fact\n",
    "            gamma_val = max(gamma_val * 0.95, 1) # Using the factor directly\n",
    "        res1 = np.linalg.norm(x_sol - y1) / max(np.linalg.norm(x_sol), 1e-16)\n",
    "        res2 = np.linalg.norm(x_sol - y2) / max(np.linalg.norm(x_sol), 1e-16)\n",
    "        if max(res1, res2) <= stop_threshold: break\n",
    "    return x_sol\n",
    "\n",
    "def project_box(x):\n",
    "    return np.clip(x, 0, 1)\n",
    "\n",
    "def project_shifted_Lp_ball(x, p):\n",
    "    shift_vec = 0.5 * np.ones_like(x)\n",
    "    shift_x = x - shift_vec\n",
    "    normp_shift = np.linalg.norm(shift_x, p)\n",
    "    n = x.size\n",
    "    if normp_shift < 1e-9: return shift_vec\n",
    "    xp = (n**(1/p)) * shift_x / (2 * normp_shift) + shift_vec\n",
    "    return xp\n",
    "\"\"\"\n",
    "\n",
    "with open(\"lpbox_admm.py\", \"w\") as f:\n",
    "    f.write(lpbox_admm_code)\n",
    "\n",
    "import lpbox_admm\n",
    "importlib.reload(lpbox_admm)\n",
    "from lpbox_admm import ADMM_bqp_linear_eq\n",
    "\n",
    "print(\"lpbox_admm.py (clean version) created and imported successfully.\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.1\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 5e-4\n",
    "EPOCHS = 100\n",
    "K_PERCENTAGE = 0.5\n",
    "K_SAMPLES = int(BATCH_SIZE * K_PERCENTAGE)\n",
    "\n",
    "EPSILON, ALPHA = 8/255, 2/255\n",
    "PGD_STEPS_TRAIN, PGD_STEPS_EVAL = 10, 20\n",
    "\n",
    "print(f\"\\nHyperparameters set. K_SAMPLES = {K_SAMPLES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4ca11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Dataset loaded. Train samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")\n",
    "print(f\"Number of training batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c5cc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_attack(model, images, labels, epsilon, alpha, iters):\n",
    "    images = images.clone().detach().to(device)\n",
    "    labels = labels.clone().detach().to(device)\n",
    "    original_images = images.clone().detach()\n",
    "    for i in range(iters):\n",
    "        images.requires_grad = True\n",
    "        outputs = model(images)\n",
    "        model.zero_grad()\n",
    "        cost = F.cross_entropy(outputs, labels)\n",
    "        cost.backward()\n",
    "        adv_images = images + alpha * images.grad.sign()\n",
    "        eta = torch.clamp(adv_images - original_images, min=-epsilon, max=epsilon)\n",
    "        images = torch.clamp(original_images + eta, min=0, max=1).detach()\n",
    "    return images\n",
    "\n",
    "def select_informed(losses, k):\n",
    "    _, indices = torch.topk(losses, k)\n",
    "    return indices\n",
    "\n",
    "class ADMM_Selection_Solver:\n",
    "    '''Wrapper class for our selection problem.'''\n",
    "    def __init__(self, n, k):\n",
    "        self.A = csc_matrix((n, n))\n",
    "        self.C = np.ones((1, n))\n",
    "        self.d = np.array([[k]], dtype=np.float64)\n",
    "        self.admm_params = {\n",
    "            'max_iters': 100,\n",
    "            'stop_threshold': 1e-3,\n",
    "            'initial_rho': 10,\n",
    "            'projection_lp': 2\n",
    "        }\n",
    "\n",
    "    def solve(self, V):\n",
    "        '''Solves the selection problem and returns the CONTINUOUS scores vector.'''\n",
    "        V_np = V.cpu().detach().numpy().reshape(-1, 1)\n",
    "        b = -V_np\n",
    "        x0 = np.random.rand(len(V), 1)\n",
    "        self.admm_params['x0'] = x0\n",
    "        \n",
    "        x_sol = ADMM_bqp_linear_eq(self.A, b, self.C, self.d, self.admm_params)\n",
    "        \n",
    "        return torch.from_numpy(x_sol.flatten()).to(V.device)\n",
    "\n",
    "admm_selection_solver = ADMM_Selection_Solver(n=2 * BATCH_SIZE, k=K_SAMPLES)\n",
    "\n",
    "def select_admm_sampler_ref(losses, k):\n",
    "    '''\n",
    "    This is the ROBUST and PURE selection function.\n",
    "    1. It gets the continuous scores (from 0 to 1) from the ADMM solver.\n",
    "    2. It selects the top 'k' samples based on these scores.\n",
    "    This guarantees that exactly 'k' samples are always selected.\n",
    "    '''\n",
    "    admm_scores = admm_selection_solver.solve(losses)    \n",
    "    _, selected_indices = torch.topk(admm_scores, k)\n",
    "    \n",
    "    return selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c170d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, model, optimizer, scheduler, history, filename):\n",
    "    '''Saves the model state to a .pth file, safely handling directory creation.'''\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'history': history\n",
    "    }\n",
    "    \n",
    "    directory = os.path.dirname(filename)\n",
    "\n",
    "    if directory:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "    torch.save(state, filename)\n",
    "    print(f\"Checkpoint saved to {filename} at epoch {epoch + 1}\")\n",
    "\n",
    "def load_checkpoint(filename, model, optimizer, scheduler):\n",
    "    '''Loads the model state from a .pth file.'''\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"No checkpoint found at {filename}. Starting training from scratch.\")\n",
    "        return 0, {'epoch': [], 'std_acc': [], 'robust_acc': [], 'time': []}\n",
    "    \n",
    "    print(f\"Attempting to load checkpoint from: {filename}\")\n",
    "    checkpoint = torch.load(filename, map_location=device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    history = checkpoint['history']\n",
    "    \n",
    "    print(f\"Checkpoint loaded successfully. Resuming training from epoch {start_epoch}.\")\n",
    "    return start_epoch, history\n",
    "\n",
    "def save_experiment_result(method_name, history_df, final_res):\n",
    "    '''Saves the final result and history to a JSON file.'''\n",
    "    safe_name = method_name.replace(' ', '_').replace('(', '').replace(')', '').replace('=', '').replace(',', '')\n",
    "    filename = f\"results_{safe_name}.json\"\n",
    "    history_json = history_df.to_json(orient='split')\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump({'history': history_json, 'final_result': final_res}, f, indent=4)\n",
    "    print(f\"\\nFinal result for '{method_name}' saved to {filename}\")\n",
    "\n",
    "def train_epoch(model, optimizer, data_loader, training_mode, selection_fn=None, k=None):\n",
    "    '''A flexible training epoch function - NO TQDM.'''\n",
    "    model.train()\n",
    "    for clean_images, labels in data_loader:\n",
    "        clean_images, labels = clean_images.to(device), labels.to(device)\n",
    "        adv_images = pgd_attack(model, clean_images, labels, EPSILON, ALPHA, PGD_STEPS_TRAIN)\n",
    "        \n",
    "        if training_mode == 'selection':\n",
    "            combined_images = torch.cat([clean_images, adv_images], dim=0)\n",
    "            combined_labels = torch.cat([labels, labels], dim=0)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(combined_images)\n",
    "                losses = F.cross_entropy(outputs, combined_labels, reduction='none')\n",
    "            selected_indices = selection_fn(losses, k)\n",
    "            final_images = combined_images[selected_indices]\n",
    "            final_labels = combined_labels[selected_indices]\n",
    "        else:\n",
    "             raise ValueError(f\"This script is configured for 'selection' mode.\")\n",
    "\n",
    "        if len(final_images) > 0:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(final_images)\n",
    "            loss = F.cross_entropy(outputs, final_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def evaluate(model, data_loader, attack_fn=None):\n",
    "    '''Evaluates the model on the given data_loader - NO TQDM.'''\n",
    "    model.eval()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for images, labels in data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        if attack_fn:\n",
    "            images = attack_fn(model, images, labels, epsilon=EPSILON, alpha=ALPHA, iters=PGD_STEPS_EVAL)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "    return 100. * total_correct / total_samples\n",
    "\n",
    "def run_experiment(method_name, training_mode, checkpoint_to_load, new_checkpoint_base, result_filename, selection_fn=None, k=None):\n",
    "    '''Runs a full training and evaluation experiment with checkpointing capability.'''\n",
    "    print(f\"\\n{'='*20} Running Experiment: {method_name} {'='*20}\")\n",
    "    \n",
    "    model = models.resnet18(weights=None, num_classes=10).to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[75, 90], gamma=0.1)\n",
    "    \n",
    "    start_epoch, history = load_checkpoint(checkpoint_to_load, model, optimizer, scheduler)\n",
    "    \n",
    "    if start_epoch >= EPOCHS:\n",
    "        print(\"Training for this method is already complete.\")\n",
    "        return\n",
    "\n",
    "    start_time = time.time() - (history['time'][-1] if history['time'] else 0)\n",
    "    \n",
    "    for epoch in range(start_epoch, EPOCHS):\n",
    "        epoch_start_time = time.time()\n",
    "        train_epoch(model, optimizer, train_loader, training_mode, selection_fn, k)\n",
    "        \n",
    "        std_acc = evaluate(model, test_loader, attack_fn=None)\n",
    "        robust_acc = evaluate(model, test_loader, attack_fn=pgd_attack)\n",
    "        scheduler.step()\n",
    "        \n",
    "        history['epoch'].append(epoch + 1)\n",
    "        history['std_acc'].append(std_acc)\n",
    "        history['robust_acc'].append(robust_acc)\n",
    "        history['time'].append(time.time() - start_time)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} -> Std Acc: {std_acc:.2f}%, Robust Acc: {robust_acc:.2f}%, Time: {epoch_time:.2f}s\")\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0 or (epoch + 1) == EPOCHS:\n",
    "            save_checkpoint(epoch, model, optimizer, scheduler, history, f\"{new_checkpoint_base}_{epoch+1}.pth\")\n",
    "\n",
    "    total_time_hours = (time.time() - start_time) / 3600\n",
    "    final_results = {\n",
    "        'Method': method_name,\n",
    "        'SA (%)': history['std_acc'][-1],\n",
    "        'RA (PGD-20, %)': history['robust_acc'][-1],\n",
    "        'Training Time (hours)': total_time_hours\n",
    "    }\n",
    "    history_df = pd.DataFrame(history)\n",
    "    save_experiment_result(method_name, history_df, final_results)    \n",
    "    print(pd.DataFrame([final_results]).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d75dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_name = f\"ADMM Sampler (k={K_SAMPLES})\"\n",
    "safe_name = method_name.replace(' ', '_').replace('(', '').replace(')', '').replace('=', '').replace(',', '')\n",
    "\n",
    "checkpoint_to_load = \"/kaggle/input/admm/pytorch/default/1/checkpoint_*.pth\"\n",
    "new_checkpoint_base_path = f\"checkpoint_{safe_name}\"\n",
    "result_filename = f\"results_{safe_name}.json\"\n",
    "\n",
    "print(f\"Preparing to RESUME experiment: {method_name} with Top-K Strategy\")\n",
    "print(f\"Loading from: {checkpoint_to_load}\")\n",
    "print(f\"Saving new checkpoints with base name: {new_checkpoint_base_path}_[epoch].pth\")\n",
    "print(f\"Final results will be saved to: {result_filename}\")\n",
    "\n",
    "if os.path.exists(result_filename):\n",
    "    print(f\"Final result file '{result_filename}' already exists. Skipping experiment.\")\n",
    "    try:\n",
    "        with open(result_filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(\"\\n--- Existing Final Results ---\")\n",
    "        print(pd.DataFrame([data['final_result']]).to_string(index=False))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read existing result file: {e}\")\n",
    "else:\n",
    "    run_experiment(\n",
    "        method_name=method_name,\n",
    "        training_mode='selection',\n",
    "        selection_fn=select_admm_sampler_ref,\n",
    "        k=K_SAMPLES,\n",
    "        checkpoint_to_load=checkpoint_to_load,\n",
    "        new_checkpoint_base=new_checkpoint_base_path,\n",
    "        result_filename=result_filename\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
